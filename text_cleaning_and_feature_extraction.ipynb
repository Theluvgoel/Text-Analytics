{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "url = \"https://nlptoolsbyluvgoel.page.gd/\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<style>@keyframes g{{0%,100%{{background-position:0 50%}}50%{{background-position:100% 50%}}}}</style>\n",
        "<div style=\"text-align:center;padding:10px;background:linear-gradient(135deg,#667eea,#764ba2,#667eea);background-size:200%;animation:g 4s ease infinite;border-radius:10px\">\n",
        "<br><h2 style=\"color:#fff;margin:0 0 5px\">ðŸ¤– NLP Tools by Luv Goel</h2><br>\n",
        "<a href=\"{url}\" target=\"_blank\" style=\"display:inline-block;padding:15px 40px;background:#fff;color:#667eea;font-weight:bold;font-size:16px;border-radius:10px;box-shadow:0 5px 15px rgba(0,0,0,.3);text-decoration:none;transition:transform .3s\" onmouseover=\"this.style.transform='scale(1.05)'\" onmouseout=\"this.style.transform='scale(1)'\">Click Here to Open Website</a>\n",
        "</div>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "gHDBoqFT3hVq",
        "outputId": "2376549b-9568-485a-b885-db4dabd31444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>@keyframes g{0%,100%{background-position:0 50%}50%{background-position:100% 50%}}</style>\n",
              "<div style=\"text-align:center;padding:10px;background:linear-gradient(135deg,#667eea,#764ba2,#667eea);background-size:200%;animation:g 4s ease infinite;border-radius:10px\">\n",
              "<br><h2 style=\"color:#fff;margin:0 0 5px\">ðŸ¤– NLP Tools by Luv Goel</h2><br>\n",
              "<a href=\"https://nlptoolsbyluvgoel.page.gd/\" target=\"_blank\" style=\"display:inline-block;padding:15px 40px;background:#fff;color:#667eea;font-weight:bold;font-size:16px;border-radius:10px;box-shadow:0 5px 15px rgba(0,0,0,.3);text-decoration:none;transition:transform .3s\" onmouseover=\"this.style.transform='scale(1.05)'\" onmouseout=\"this.style.transform='scale(1)'\">Click Here to Open Website</a>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjbzRtfJ-sa1",
        "outputId": "919f2279-b1bc-46df-a96f-e0c1e78de29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kniOTrWd-_D2",
        "outputId": "5c4cc704-9fb3-4675-f7de-939a6729f562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "O0xeTBYeobIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPpkZlgrpdsB",
        "outputId": "d02dac39-620c-4ae4-909b-db9d4559b0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('Introduction to Machine Learning.pdf','rb')\n",
        "\n",
        "# Sample documents\n",
        "documents = PyPDF2.PdfReader(f)"
      ],
      "metadata": {
        "id": "SRoszuDkpkl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEXT PREPROCESSING METHODS**\n",
        "\n"
      ],
      "metadata": {
        "id": "K6p1GkDqqVDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take first document as example\n",
        "text = documents.pages[0].extract_text()\n",
        "print(f\"\\nOriginal Text: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSmugkr_pqQB",
        "outputId": "34e089db-420a-42b9-da1d-6cae5707241b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Text: Machine learning and artificial intelligence are rapidly transforming the modern digital \n",
            "world. These technologies enable computer systems to learn from data, recognize patterns, \n",
            "and make intelligent decisions without being explicitly programmed. As data continues to \n",
            "grow exponentially, machine learning has become one of the most important tools for \n",
            "extracting meaningful insights and building intelligent applications.  \n",
            "I love machine learning and artificial intelligence because they open  endless possibilities for \n",
            "innovation. From healthcare and finance to transportation and entertainment, machine \n",
            "learning systems are being used to solve complex real -world problems efficiently. Tasks that \n",
            "once required human intelligence, such as image rec ognition, speech processing, and \n",
            "recommendation systems, can now be performed accurately by machines.  \n",
            "Machine learning is amazing for data analysis because it can process large volumes of \n",
            "structured and unstructured data quickly. Traditional data analysis methods often struggle \n",
            "with scale and complexity, but machine learning algorithms can automatically identify hidden \n",
            "patterns, trends, and relationships within data. This makes them extremely valuable for \n",
            "prediction, classification, and decision -making task s. \n",
            "Natural language processing is a significant branch of artificial intelligence that heavily relies \n",
            "on machine learning techniques. It focuses on enabling machines to understand, interpret, and \n",
            "generate human language. Applications such as chatbots, virt ual assistants, language \n",
            "translation, and sentiment analysis are powered by natural language processing models \n",
            "trained using machine learning algorithms.  \n",
            "In conclusion, machine learning and artificial intelligence are shaping the future of \n",
            "technology. Thei r ability to learn from data, adapt to new information, and improve over time \n",
            "makes them essential tools in todayâ€™s data -driven world. As research and development \n",
            "continue, machine learning will play an even greater role in advancing intelligent systems an d \n",
            "improving everyday life.  \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenization"
      ],
      "metadata": {
        "id": "UzGnT2WvpsMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TOKENIZATION (breaking text into words)\")\n",
        "tokens = word_tokenize(text)\n",
        "print(f\"   Result: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_cCYVnDpzi3",
        "outputId": "5661d995-8d85-4185-c8b5-04a16c7f5513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENIZATION (breaking text into words)\n",
            "   Result: ['Machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'rapidly', 'transforming', 'the', 'modern', 'digital', 'world', '.', 'These', 'technologies', 'enable', 'computer', 'systems', 'to', 'learn', 'from', 'data', ',', 'recognize', 'patterns', ',', 'and', 'make', 'intelligent', 'decisions', 'without', 'being', 'explicitly', 'programmed', '.', 'As', 'data', 'continues', 'to', 'grow', 'exponentially', ',', 'machine', 'learning', 'has', 'become', 'one', 'of', 'the', 'most', 'important', 'tools', 'for', 'extracting', 'meaningful', 'insights', 'and', 'building', 'intelligent', 'applications', '.', 'I', 'love', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'because', 'they', 'open', 'endless', 'possibilities', 'for', 'innovation', '.', 'From', 'healthcare', 'and', 'finance', 'to', 'transportation', 'and', 'entertainment', ',', 'machine', 'learning', 'systems', 'are', 'being', 'used', 'to', 'solve', 'complex', 'real', '-world', 'problems', 'efficiently', '.', 'Tasks', 'that', 'once', 'required', 'human', 'intelligence', ',', 'such', 'as', 'image', 'rec', 'ognition', ',', 'speech', 'processing', ',', 'and', 'recommendation', 'systems', ',', 'can', 'now', 'be', 'performed', 'accurately', 'by', 'machines', '.', 'Machine', 'learning', 'is', 'amazing', 'for', 'data', 'analysis', 'because', 'it', 'can', 'process', 'large', 'volumes', 'of', 'structured', 'and', 'unstructured', 'data', 'quickly', '.', 'Traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'with', 'scale', 'and', 'complexity', ',', 'but', 'machine', 'learning', 'algorithms', 'can', 'automatically', 'identify', 'hidden', 'patterns', ',', 'trends', ',', 'and', 'relationships', 'within', 'data', '.', 'This', 'makes', 'them', 'extremely', 'valuable', 'for', 'prediction', ',', 'classification', ',', 'and', 'decision', '-making', 'task', 's.', 'Natural', 'language', 'processing', 'is', 'a', 'significant', 'branch', 'of', 'artificial', 'intelligence', 'that', 'heavily', 'relies', 'on', 'machine', 'learning', 'techniques', '.', 'It', 'focuses', 'on', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'such', 'as', 'chatbots', ',', 'virt', 'ual', 'assistants', ',', 'language', 'translation', ',', 'and', 'sentiment', 'analysis', 'are', 'powered', 'by', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', '.', 'In', 'conclusion', ',', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'shaping', 'the', 'future', 'of', 'technology', '.', 'Thei', 'r', 'ability', 'to', 'learn', 'from', 'data', ',', 'adapt', 'to', 'new', 'information', ',', 'and', 'improve', 'over', 'time', 'makes', 'them', 'essential', 'tools', 'in', 'today', 'â€™', 's', 'data', '-driven', 'world', '.', 'As', 'research', 'and', 'development', 'continue', ',', 'machine', 'learning', 'will', 'play', 'an', 'even', 'greater', 'role', 'in', 'advancing', 'intelligent', 'systems', 'an', 'd', 'improving', 'everyday', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Convert To Lowercase"
      ],
      "metadata": {
        "id": "hPPlYIGtqnKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LOWERCASE\")\n",
        "tokens_lower = [word.lower() for word in tokens]\n",
        "print(f\"   Result: {tokens_lower}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk1fhDovqzF9",
        "outputId": "82970838-7d77-41ee-b9b6-b2d847ad7fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOWERCASE\n",
            "   Result: ['machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'rapidly', 'transforming', 'the', 'modern', 'digital', 'world', '.', 'these', 'technologies', 'enable', 'computer', 'systems', 'to', 'learn', 'from', 'data', ',', 'recognize', 'patterns', ',', 'and', 'make', 'intelligent', 'decisions', 'without', 'being', 'explicitly', 'programmed', '.', 'as', 'data', 'continues', 'to', 'grow', 'exponentially', ',', 'machine', 'learning', 'has', 'become', 'one', 'of', 'the', 'most', 'important', 'tools', 'for', 'extracting', 'meaningful', 'insights', 'and', 'building', 'intelligent', 'applications', '.', 'i', 'love', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'because', 'they', 'open', 'endless', 'possibilities', 'for', 'innovation', '.', 'from', 'healthcare', 'and', 'finance', 'to', 'transportation', 'and', 'entertainment', ',', 'machine', 'learning', 'systems', 'are', 'being', 'used', 'to', 'solve', 'complex', 'real', '-world', 'problems', 'efficiently', '.', 'tasks', 'that', 'once', 'required', 'human', 'intelligence', ',', 'such', 'as', 'image', 'rec', 'ognition', ',', 'speech', 'processing', ',', 'and', 'recommendation', 'systems', ',', 'can', 'now', 'be', 'performed', 'accurately', 'by', 'machines', '.', 'machine', 'learning', 'is', 'amazing', 'for', 'data', 'analysis', 'because', 'it', 'can', 'process', 'large', 'volumes', 'of', 'structured', 'and', 'unstructured', 'data', 'quickly', '.', 'traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'with', 'scale', 'and', 'complexity', ',', 'but', 'machine', 'learning', 'algorithms', 'can', 'automatically', 'identify', 'hidden', 'patterns', ',', 'trends', ',', 'and', 'relationships', 'within', 'data', '.', 'this', 'makes', 'them', 'extremely', 'valuable', 'for', 'prediction', ',', 'classification', ',', 'and', 'decision', '-making', 'task', 's.', 'natural', 'language', 'processing', 'is', 'a', 'significant', 'branch', 'of', 'artificial', 'intelligence', 'that', 'heavily', 'relies', 'on', 'machine', 'learning', 'techniques', '.', 'it', 'focuses', 'on', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'applications', 'such', 'as', 'chatbots', ',', 'virt', 'ual', 'assistants', ',', 'language', 'translation', ',', 'and', 'sentiment', 'analysis', 'are', 'powered', 'by', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', '.', 'in', 'conclusion', ',', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'shaping', 'the', 'future', 'of', 'technology', '.', 'thei', 'r', 'ability', 'to', 'learn', 'from', 'data', ',', 'adapt', 'to', 'new', 'information', ',', 'and', 'improve', 'over', 'time', 'makes', 'them', 'essential', 'tools', 'in', 'today', 'â€™', 's', 'data', '-driven', 'world', '.', 'as', 'research', 'and', 'development', 'continue', ',', 'machine', 'learning', 'will', 'play', 'an', 'even', 'greater', 'role', 'in', 'advancing', 'intelligent', 'systems', 'an', 'd', 'improving', 'everyday', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Remove Stopwords\n",
        "\n"
      ],
      "metadata": {
        "id": "dGH9f3uYq0dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STOPWORD REMOVAL (remove common words like 'and', 'is', 'the')\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stop = [word for word in tokens_lower if word.isalpha() and word not in stop_words]\n",
        "print(f\"   Before: {tokens_lower}\")\n",
        "print(f\"   After:  {tokens_no_stop}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBBo8srkq6ql",
        "outputId": "48d8c0d4-b2ce-4268-fca7-701a62ac9a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOPWORD REMOVAL (remove common words like 'and', 'is', 'the')\n",
            "   Before: ['machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'rapidly', 'transforming', 'the', 'modern', 'digital', 'world', '.', 'these', 'technologies', 'enable', 'computer', 'systems', 'to', 'learn', 'from', 'data', ',', 'recognize', 'patterns', ',', 'and', 'make', 'intelligent', 'decisions', 'without', 'being', 'explicitly', 'programmed', '.', 'as', 'data', 'continues', 'to', 'grow', 'exponentially', ',', 'machine', 'learning', 'has', 'become', 'one', 'of', 'the', 'most', 'important', 'tools', 'for', 'extracting', 'meaningful', 'insights', 'and', 'building', 'intelligent', 'applications', '.', 'i', 'love', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'because', 'they', 'open', 'endless', 'possibilities', 'for', 'innovation', '.', 'from', 'healthcare', 'and', 'finance', 'to', 'transportation', 'and', 'entertainment', ',', 'machine', 'learning', 'systems', 'are', 'being', 'used', 'to', 'solve', 'complex', 'real', '-world', 'problems', 'efficiently', '.', 'tasks', 'that', 'once', 'required', 'human', 'intelligence', ',', 'such', 'as', 'image', 'rec', 'ognition', ',', 'speech', 'processing', ',', 'and', 'recommendation', 'systems', ',', 'can', 'now', 'be', 'performed', 'accurately', 'by', 'machines', '.', 'machine', 'learning', 'is', 'amazing', 'for', 'data', 'analysis', 'because', 'it', 'can', 'process', 'large', 'volumes', 'of', 'structured', 'and', 'unstructured', 'data', 'quickly', '.', 'traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'with', 'scale', 'and', 'complexity', ',', 'but', 'machine', 'learning', 'algorithms', 'can', 'automatically', 'identify', 'hidden', 'patterns', ',', 'trends', ',', 'and', 'relationships', 'within', 'data', '.', 'this', 'makes', 'them', 'extremely', 'valuable', 'for', 'prediction', ',', 'classification', ',', 'and', 'decision', '-making', 'task', 's.', 'natural', 'language', 'processing', 'is', 'a', 'significant', 'branch', 'of', 'artificial', 'intelligence', 'that', 'heavily', 'relies', 'on', 'machine', 'learning', 'techniques', '.', 'it', 'focuses', 'on', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'applications', 'such', 'as', 'chatbots', ',', 'virt', 'ual', 'assistants', ',', 'language', 'translation', ',', 'and', 'sentiment', 'analysis', 'are', 'powered', 'by', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', '.', 'in', 'conclusion', ',', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'are', 'shaping', 'the', 'future', 'of', 'technology', '.', 'thei', 'r', 'ability', 'to', 'learn', 'from', 'data', ',', 'adapt', 'to', 'new', 'information', ',', 'and', 'improve', 'over', 'time', 'makes', 'them', 'essential', 'tools', 'in', 'today', 'â€™', 's', 'data', '-driven', 'world', '.', 'as', 'research', 'and', 'development', 'continue', ',', 'machine', 'learning', 'will', 'play', 'an', 'even', 'greater', 'role', 'in', 'advancing', 'intelligent', 'systems', 'an', 'd', 'improving', 'everyday', 'life', '.']\n",
            "   After:  ['machine', 'learning', 'artificial', 'intelligence', 'rapidly', 'transforming', 'modern', 'digital', 'world', 'technologies', 'enable', 'computer', 'systems', 'learn', 'data', 'recognize', 'patterns', 'make', 'intelligent', 'decisions', 'without', 'explicitly', 'programmed', 'data', 'continues', 'grow', 'exponentially', 'machine', 'learning', 'become', 'one', 'important', 'tools', 'extracting', 'meaningful', 'insights', 'building', 'intelligent', 'applications', 'love', 'machine', 'learning', 'artificial', 'intelligence', 'open', 'endless', 'possibilities', 'innovation', 'healthcare', 'finance', 'transportation', 'entertainment', 'machine', 'learning', 'systems', 'used', 'solve', 'complex', 'real', 'problems', 'efficiently', 'tasks', 'required', 'human', 'intelligence', 'image', 'rec', 'ognition', 'speech', 'processing', 'recommendation', 'systems', 'performed', 'accurately', 'machines', 'machine', 'learning', 'amazing', 'data', 'analysis', 'process', 'large', 'volumes', 'structured', 'unstructured', 'data', 'quickly', 'traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'scale', 'complexity', 'machine', 'learning', 'algorithms', 'automatically', 'identify', 'hidden', 'patterns', 'trends', 'relationships', 'within', 'data', 'makes', 'extremely', 'valuable', 'prediction', 'classification', 'decision', 'task', 'natural', 'language', 'processing', 'significant', 'branch', 'artificial', 'intelligence', 'heavily', 'relies', 'machine', 'learning', 'techniques', 'focuses', 'enabling', 'machines', 'understand', 'interpret', 'generate', 'human', 'language', 'applications', 'chatbots', 'virt', 'ual', 'assistants', 'language', 'translation', 'sentiment', 'analysis', 'powered', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', 'conclusion', 'machine', 'learning', 'artificial', 'intelligence', 'shaping', 'future', 'technology', 'thei', 'r', 'ability', 'learn', 'data', 'adapt', 'new', 'information', 'improve', 'time', 'makes', 'essential', 'tools', 'today', 'data', 'world', 'research', 'development', 'continue', 'machine', 'learning', 'play', 'even', 'greater', 'role', 'advancing', 'intelligent', 'systems', 'improving', 'everyday', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Stemming"
      ],
      "metadata": {
        "id": "qP7S6o8QrFIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEMMING (reduce words to root form - faster but rough)\")\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "print(f\"   Original: {tokens_no_stop}\")\n",
        "print(f\"   Stemmed:  {stemmed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh-6-SBtq-5e",
        "outputId": "ac18562f-173c-43a1-cd32-0e1749d89ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEMMING (reduce words to root form - faster but rough)\n",
            "   Original: ['machine', 'learning', 'artificial', 'intelligence', 'rapidly', 'transforming', 'modern', 'digital', 'world', 'technologies', 'enable', 'computer', 'systems', 'learn', 'data', 'recognize', 'patterns', 'make', 'intelligent', 'decisions', 'without', 'explicitly', 'programmed', 'data', 'continues', 'grow', 'exponentially', 'machine', 'learning', 'become', 'one', 'important', 'tools', 'extracting', 'meaningful', 'insights', 'building', 'intelligent', 'applications', 'love', 'machine', 'learning', 'artificial', 'intelligence', 'open', 'endless', 'possibilities', 'innovation', 'healthcare', 'finance', 'transportation', 'entertainment', 'machine', 'learning', 'systems', 'used', 'solve', 'complex', 'real', 'problems', 'efficiently', 'tasks', 'required', 'human', 'intelligence', 'image', 'rec', 'ognition', 'speech', 'processing', 'recommendation', 'systems', 'performed', 'accurately', 'machines', 'machine', 'learning', 'amazing', 'data', 'analysis', 'process', 'large', 'volumes', 'structured', 'unstructured', 'data', 'quickly', 'traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'scale', 'complexity', 'machine', 'learning', 'algorithms', 'automatically', 'identify', 'hidden', 'patterns', 'trends', 'relationships', 'within', 'data', 'makes', 'extremely', 'valuable', 'prediction', 'classification', 'decision', 'task', 'natural', 'language', 'processing', 'significant', 'branch', 'artificial', 'intelligence', 'heavily', 'relies', 'machine', 'learning', 'techniques', 'focuses', 'enabling', 'machines', 'understand', 'interpret', 'generate', 'human', 'language', 'applications', 'chatbots', 'virt', 'ual', 'assistants', 'language', 'translation', 'sentiment', 'analysis', 'powered', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', 'conclusion', 'machine', 'learning', 'artificial', 'intelligence', 'shaping', 'future', 'technology', 'thei', 'r', 'ability', 'learn', 'data', 'adapt', 'new', 'information', 'improve', 'time', 'makes', 'essential', 'tools', 'today', 'data', 'world', 'research', 'development', 'continue', 'machine', 'learning', 'play', 'even', 'greater', 'role', 'advancing', 'intelligent', 'systems', 'improving', 'everyday', 'life']\n",
            "   Stemmed:  ['machin', 'learn', 'artifici', 'intellig', 'rapidli', 'transform', 'modern', 'digit', 'world', 'technolog', 'enabl', 'comput', 'system', 'learn', 'data', 'recogn', 'pattern', 'make', 'intellig', 'decis', 'without', 'explicitli', 'program', 'data', 'continu', 'grow', 'exponenti', 'machin', 'learn', 'becom', 'one', 'import', 'tool', 'extract', 'meaning', 'insight', 'build', 'intellig', 'applic', 'love', 'machin', 'learn', 'artifici', 'intellig', 'open', 'endless', 'possibl', 'innov', 'healthcar', 'financ', 'transport', 'entertain', 'machin', 'learn', 'system', 'use', 'solv', 'complex', 'real', 'problem', 'effici', 'task', 'requir', 'human', 'intellig', 'imag', 'rec', 'ognit', 'speech', 'process', 'recommend', 'system', 'perform', 'accur', 'machin', 'machin', 'learn', 'amaz', 'data', 'analysi', 'process', 'larg', 'volum', 'structur', 'unstructur', 'data', 'quickli', 'tradit', 'data', 'analysi', 'method', 'often', 'struggl', 'scale', 'complex', 'machin', 'learn', 'algorithm', 'automat', 'identifi', 'hidden', 'pattern', 'trend', 'relationship', 'within', 'data', 'make', 'extrem', 'valuabl', 'predict', 'classif', 'decis', 'task', 'natur', 'languag', 'process', 'signific', 'branch', 'artifici', 'intellig', 'heavili', 'reli', 'machin', 'learn', 'techniqu', 'focus', 'enabl', 'machin', 'understand', 'interpret', 'gener', 'human', 'languag', 'applic', 'chatbot', 'virt', 'ual', 'assist', 'languag', 'translat', 'sentiment', 'analysi', 'power', 'natur', 'languag', 'process', 'model', 'train', 'use', 'machin', 'learn', 'algorithm', 'conclus', 'machin', 'learn', 'artifici', 'intellig', 'shape', 'futur', 'technolog', 'thei', 'r', 'abil', 'learn', 'data', 'adapt', 'new', 'inform', 'improv', 'time', 'make', 'essenti', 'tool', 'today', 'data', 'world', 'research', 'develop', 'continu', 'machin', 'learn', 'play', 'even', 'greater', 'role', 'advanc', 'intellig', 'system', 'improv', 'everyday', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with more words\n",
        "example_words = ['running', 'runs', 'ran']\n",
        "print(f\"More examples:\")\n",
        "for word in example_words:\n",
        "    print(f\"   {word} -> {stemmer.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5FongBLrNzP",
        "outputId": "317afafd-1ef2-425f-f581-6e22ff10ca84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "More examples:\n",
            "   running -> run\n",
            "   runs -> run\n",
            "   ran -> ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Lemmatization\n"
      ],
      "metadata": {
        "id": "elcrtgFGrPyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LEMMATIZATION (reduce to dictionary form - slower but accurate)\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in tokens_no_stop]\n",
        "print(f\"   Original:    {tokens_no_stop}\")\n",
        "print(f\"   Lemmatized:  {lemmatized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljPl04aIrU8P",
        "outputId": "0fd9898d-c782-48ca-bb25-c59ce5b53158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEMMATIZATION (reduce to dictionary form - slower but accurate)\n",
            "   Original:    ['machine', 'learning', 'artificial', 'intelligence', 'rapidly', 'transforming', 'modern', 'digital', 'world', 'technologies', 'enable', 'computer', 'systems', 'learn', 'data', 'recognize', 'patterns', 'make', 'intelligent', 'decisions', 'without', 'explicitly', 'programmed', 'data', 'continues', 'grow', 'exponentially', 'machine', 'learning', 'become', 'one', 'important', 'tools', 'extracting', 'meaningful', 'insights', 'building', 'intelligent', 'applications', 'love', 'machine', 'learning', 'artificial', 'intelligence', 'open', 'endless', 'possibilities', 'innovation', 'healthcare', 'finance', 'transportation', 'entertainment', 'machine', 'learning', 'systems', 'used', 'solve', 'complex', 'real', 'problems', 'efficiently', 'tasks', 'required', 'human', 'intelligence', 'image', 'rec', 'ognition', 'speech', 'processing', 'recommendation', 'systems', 'performed', 'accurately', 'machines', 'machine', 'learning', 'amazing', 'data', 'analysis', 'process', 'large', 'volumes', 'structured', 'unstructured', 'data', 'quickly', 'traditional', 'data', 'analysis', 'methods', 'often', 'struggle', 'scale', 'complexity', 'machine', 'learning', 'algorithms', 'automatically', 'identify', 'hidden', 'patterns', 'trends', 'relationships', 'within', 'data', 'makes', 'extremely', 'valuable', 'prediction', 'classification', 'decision', 'task', 'natural', 'language', 'processing', 'significant', 'branch', 'artificial', 'intelligence', 'heavily', 'relies', 'machine', 'learning', 'techniques', 'focuses', 'enabling', 'machines', 'understand', 'interpret', 'generate', 'human', 'language', 'applications', 'chatbots', 'virt', 'ual', 'assistants', 'language', 'translation', 'sentiment', 'analysis', 'powered', 'natural', 'language', 'processing', 'models', 'trained', 'using', 'machine', 'learning', 'algorithms', 'conclusion', 'machine', 'learning', 'artificial', 'intelligence', 'shaping', 'future', 'technology', 'thei', 'r', 'ability', 'learn', 'data', 'adapt', 'new', 'information', 'improve', 'time', 'makes', 'essential', 'tools', 'today', 'data', 'world', 'research', 'development', 'continue', 'machine', 'learning', 'play', 'even', 'greater', 'role', 'advancing', 'intelligent', 'systems', 'improving', 'everyday', 'life']\n",
            "   Lemmatized:  ['machine', 'learning', 'artificial', 'intelligence', 'rapidly', 'transforming', 'modern', 'digital', 'world', 'technology', 'enable', 'computer', 'system', 'learn', 'data', 'recognize', 'pattern', 'make', 'intelligent', 'decision', 'without', 'explicitly', 'programmed', 'data', 'continues', 'grow', 'exponentially', 'machine', 'learning', 'become', 'one', 'important', 'tool', 'extracting', 'meaningful', 'insight', 'building', 'intelligent', 'application', 'love', 'machine', 'learning', 'artificial', 'intelligence', 'open', 'endless', 'possibility', 'innovation', 'healthcare', 'finance', 'transportation', 'entertainment', 'machine', 'learning', 'system', 'used', 'solve', 'complex', 'real', 'problem', 'efficiently', 'task', 'required', 'human', 'intelligence', 'image', 'rec', 'ognition', 'speech', 'processing', 'recommendation', 'system', 'performed', 'accurately', 'machine', 'machine', 'learning', 'amazing', 'data', 'analysis', 'process', 'large', 'volume', 'structured', 'unstructured', 'data', 'quickly', 'traditional', 'data', 'analysis', 'method', 'often', 'struggle', 'scale', 'complexity', 'machine', 'learning', 'algorithm', 'automatically', 'identify', 'hidden', 'pattern', 'trend', 'relationship', 'within', 'data', 'make', 'extremely', 'valuable', 'prediction', 'classification', 'decision', 'task', 'natural', 'language', 'processing', 'significant', 'branch', 'artificial', 'intelligence', 'heavily', 'relies', 'machine', 'learning', 'technique', 'focus', 'enabling', 'machine', 'understand', 'interpret', 'generate', 'human', 'language', 'application', 'chatbots', 'virt', 'ual', 'assistant', 'language', 'translation', 'sentiment', 'analysis', 'powered', 'natural', 'language', 'processing', 'model', 'trained', 'using', 'machine', 'learning', 'algorithm', 'conclusion', 'machine', 'learning', 'artificial', 'intelligence', 'shaping', 'future', 'technology', 'thei', 'r', 'ability', 'learn', 'data', 'adapt', 'new', 'information', 'improve', 'time', 'make', 'essential', 'tool', 'today', 'data', 'world', 'research', 'development', 'continue', 'machine', 'learning', 'play', 'even', 'greater', 'role', 'advancing', 'intelligent', 'system', 'improving', 'everyday', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"More examples:\")\n",
        "for word in example_words:\n",
        "    print(f\"   {word} -> {lemmatizer.lemmatize(word, pos='v')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhfL-8EcrW4z",
        "outputId": "c72785f8-3bb8-4ae5-e19d-8d31ebc36e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "More examples:\n",
            "   running -> run\n",
            "   runs -> run\n",
            "   ran -> run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparison : Stemming vs Lematization"
      ],
      "metadata": {
        "id": "YZ0IzR2vrbqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word        | Stemming | Lemmatization\")\n",
        "print(\"-\"*45)\n",
        "for word in ['imporve', 'was', 'caring']:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    print(f\"{word:11} | {stem:8} | {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaaWL0z4rh1C",
        "outputId": "def9f261-7639-421a-e3d3-bdcef55d9a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word        | Stemming | Lemmatization\n",
            "---------------------------------------------\n",
            "imporve     | imporv   | imporve\n",
            "was         | wa       | be\n",
            "caring      | care     | care\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cleaned documents for next part\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "cleaned_docs = [preprocess(page.extract_text()) for page in documents.pages]\n",
        "\n",
        "print(\"Cleaned Documents (after all preprocessing):\")\n",
        "for i, doc in enumerate(cleaned_docs, 1):\n",
        "    print(f\"{i}. {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86y3TG2Lr3K-",
        "outputId": "088b766a-2b0a-4ead-871e-01451d6f2227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Documents (after all preprocessing):\n",
            "1. machine learning artificial intelligence rapidly transforming modern digital world technology enable computer system learn data recognize pattern make intelligent decision without explicitly programmed data continues grow exponentially machine learning become one important tool extracting meaningful insight building intelligent application love machine learning artificial intelligence open endless possibility innovation healthcare finance transportation entertainment machine learning system used solve complex real problem efficiently task required human intelligence image rec ognition speech processing recommendation system performed accurately machine machine learning amazing data analysis process large volume structured unstructured data quickly traditional data analysis method often struggle scale complexity machine learning algorithm automatically identify hidden pattern trend relationship within data make extremely valuable prediction classification decision task natural language processing significant branch artificial intelligence heavily relies machine learning technique focus enabling machine understand interpret generate human language application chatbots virt ual assistant language translation sentiment analysis powered natural language processing model trained using machine learning algorithm conclusion machine learning artificial intelligence shaping future technology thei r ability learn data adapt new information improve time make essential tool today data world research development continue machine learning play even greater role advancing intelligent system improving everyday life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEXT REPRESENTATION MODELS**"
      ],
      "metadata": {
        "id": "55hGKzXTsC6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. BAG OF WORDS (BoW)\n"
      ],
      "metadata": {
        "id": "D2Y8esmDsGb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*70)\n",
        "print(\"BAG-OF-WORDS (Count how many times each word appears)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(cleaned_docs)\n",
        "\n",
        "print(f\"\\nVocabulary (unique words): {bow_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"\\nBoW Matrix (rows=documents, columns=words):\")\n",
        "print(bow_matrix.toarray())\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- Each number = how many times that word appears in that document\")\n",
        "print(\"- Example: If 'machine' appears 2 times in doc 1, cell value = 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwB_dzYTsLwh",
        "outputId": "b5bb28e6-4e91-4c23-9763-493b1aa84c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "BAG-OF-WORDS (Count how many times each word appears)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Vocabulary (unique words): ['ability' 'accurately' 'adapt' 'advancing' 'algorithm' 'amazing'\n",
            " 'analysis' 'application' 'artificial' 'assistant' 'automatically'\n",
            " 'become' 'branch' 'building' 'chatbots' 'classification' 'complex'\n",
            " 'complexity' 'computer' 'conclusion' 'continue' 'continues' 'data'\n",
            " 'decision' 'development' 'digital' 'efficiently' 'enable' 'enabling'\n",
            " 'endless' 'entertainment' 'essential' 'even' 'everyday' 'explicitly'\n",
            " 'exponentially' 'extracting' 'extremely' 'finance' 'focus' 'future'\n",
            " 'generate' 'greater' 'grow' 'healthcare' 'heavily' 'hidden' 'human'\n",
            " 'identify' 'image' 'important' 'improve' 'improving' 'information'\n",
            " 'innovation' 'insight' 'intelligence' 'intelligent' 'interpret'\n",
            " 'language' 'large' 'learn' 'learning' 'life' 'love' 'machine' 'make'\n",
            " 'meaningful' 'method' 'model' 'modern' 'natural' 'new' 'often' 'ognition'\n",
            " 'one' 'open' 'pattern' 'performed' 'play' 'possibility' 'powered'\n",
            " 'prediction' 'problem' 'process' 'processing' 'programmed' 'quickly'\n",
            " 'rapidly' 'real' 'rec' 'recognize' 'recommendation' 'relationship'\n",
            " 'relies' 'required' 'research' 'role' 'scale' 'sentiment' 'shaping'\n",
            " 'significant' 'solve' 'speech' 'structured' 'struggle' 'system' 'task'\n",
            " 'technique' 'technology' 'thei' 'time' 'today' 'tool' 'traditional'\n",
            " 'trained' 'transforming' 'translation' 'transportation' 'trend' 'ual'\n",
            " 'understand' 'unstructured' 'used' 'using' 'valuable' 'virt' 'volume'\n",
            " 'within' 'without' 'world']\n",
            "\n",
            "BoW Matrix (rows=documents, columns=words):\n",
            "[[ 1  1  1  1  2  1  3  2  4  1  1  1  1  1  1  1  1  1  1  1  1  1  8  2\n",
            "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
            "   1  1  1  1  1  1  1  1  5  3  1  4  1  2 10  1  1 12  3  1  1  1  1  2\n",
            "   1  1  1  1  1  2  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  4  2  1  2  1  1  1  2  1  1  1  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  2]]\n",
            "\n",
            "Explanation:\n",
            "- Each number = how many times that word appears in that document\n",
            "- Example: If 'machine' appears 2 times in doc 1, cell value = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. TF-IDF"
      ],
      "metadata": {
        "id": "-6H7agjTsQjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*70)\n",
        "print(\"TF-IDF (Gives importance to rare words)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_docs)\n",
        "\n",
        "print(f\"\\nVocabulary: {tfidf_vectorizer.get_feature_names_out()}\")\n",
        "print(f\"\\nTF-IDF Matrix:\")\n",
        "print(np.round(tfidf_matrix.toarray(), 2))\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- Higher values = word is important in this document but rare overall\")\n",
        "print(\"- Common words (like 'machine' in all docs) get lower scores\")\n",
        "print(\"- Unique words (like 'love' in only doc 1) get higher scores\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJCyPTksXNW",
        "outputId": "ed2b545e-e2dd-4494-ab3b-c842ad8be124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "TF-IDF (Gives importance to rare words)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Vocabulary: ['ability' 'accurately' 'adapt' 'advancing' 'algorithm' 'amazing'\n",
            " 'analysis' 'application' 'artificial' 'assistant' 'automatically'\n",
            " 'become' 'branch' 'building' 'chatbots' 'classification' 'complex'\n",
            " 'complexity' 'computer' 'conclusion' 'continue' 'continues' 'data'\n",
            " 'decision' 'development' 'digital' 'efficiently' 'enable' 'enabling'\n",
            " 'endless' 'entertainment' 'essential' 'even' 'everyday' 'explicitly'\n",
            " 'exponentially' 'extracting' 'extremely' 'finance' 'focus' 'future'\n",
            " 'generate' 'greater' 'grow' 'healthcare' 'heavily' 'hidden' 'human'\n",
            " 'identify' 'image' 'important' 'improve' 'improving' 'information'\n",
            " 'innovation' 'insight' 'intelligence' 'intelligent' 'interpret'\n",
            " 'language' 'large' 'learn' 'learning' 'life' 'love' 'machine' 'make'\n",
            " 'meaningful' 'method' 'model' 'modern' 'natural' 'new' 'often' 'ognition'\n",
            " 'one' 'open' 'pattern' 'performed' 'play' 'possibility' 'powered'\n",
            " 'prediction' 'problem' 'process' 'processing' 'programmed' 'quickly'\n",
            " 'rapidly' 'real' 'rec' 'recognize' 'recommendation' 'relationship'\n",
            " 'relies' 'required' 'research' 'role' 'scale' 'sentiment' 'shaping'\n",
            " 'significant' 'solve' 'speech' 'structured' 'struggle' 'system' 'task'\n",
            " 'technique' 'technology' 'thei' 'time' 'today' 'tool' 'traditional'\n",
            " 'trained' 'transforming' 'translation' 'transportation' 'trend' 'ual'\n",
            " 'understand' 'unstructured' 'used' 'using' 'valuable' 'virt' 'volume'\n",
            " 'within' 'without' 'world']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.04 0.04 0.04 0.04 0.08 0.04 0.13 0.08 0.17 0.04 0.04 0.04 0.04 0.04\n",
            "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.34 0.08 0.04 0.04 0.04 0.04\n",
            "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
            "  0.04 0.04 0.04 0.04 0.04 0.08 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
            "  0.21 0.13 0.04 0.17 0.04 0.08 0.42 0.04 0.04 0.5  0.13 0.04 0.04 0.04\n",
            "  0.04 0.08 0.04 0.04 0.04 0.04 0.04 0.08 0.04 0.04 0.04 0.04 0.04 0.04\n",
            "  0.04 0.13 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
            "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.17 0.08 0.04 0.08 0.04 0.04\n",
            "  0.04 0.08 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
            "  0.04 0.04 0.04 0.04 0.08]]\n",
            "\n",
            "Explanation:\n",
            "- Higher values = word is important in this document but rare overall\n",
            "- Common words (like 'machine' in all docs) get lower scores\n",
            "- Unique words (like 'love' in only doc 1) get higher scores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. WORD2VEC"
      ],
      "metadata": {
        "id": "N9CmHekAsaMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*70)\n",
        "print(\"WORD2VEC (Words as dense vectors capturing meaning)\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Prepare data for Word2Vec\n",
        "tokenized_docs = [doc.split() for doc in cleaned_docs]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs,\n",
        "                     vector_size=10,  # Small size for clarity\n",
        "                     window=2,\n",
        "                     min_count=1,\n",
        "                     epochs=100)\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(w2v_model.wv)} words\")\n",
        "print(f\"Each word = vector of {w2v_model.vector_size} numbers\")\n",
        "\n",
        "# Show some word vectors\n",
        "print(\"\\nExample word vectors (first 5 dimensions shown):\")\n",
        "for word in ['machine', 'learning', 'love'][:3]:\n",
        "    if word in w2v_model.wv:\n",
        "        vector = w2v_model.wv[word][:5]\n",
        "        print(f\"{word:10} -> {np.round(vector, 2)}\")\n",
        "\n",
        "# Word similarity\n",
        "print(\"\\nWord Similarity (similar words have similar vectors):\")\n",
        "if 'machine' in w2v_model.wv and 'learning' in w2v_model.wv:\n",
        "    sim = w2v_model.wv.similarity('machine', 'learning')\n",
        "    print(f\"'machine' and 'learning': {sim:.3f} (closer to 1 = more similar)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-EmaB-st6KX",
        "outputId": "8019664d-482b-489d-f5cb-55dc75b564cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "WORD2VEC (Words as dense vectors capturing meaning)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Vocabulary size: 132 words\n",
            "Each word = vector of 10 numbers\n",
            "\n",
            "Example word vectors (first 5 dimensions shown):\n",
            "machine    -> [ 0.    0.1   0.42  0.17 -0.1 ]\n",
            "learning   -> [ 0.08  0.07  0.27  0.13 -0.05]\n",
            "love       -> [ 0.06  0.06  0.09 -0.    0.05]\n",
            "\n",
            "Word Similarity (similar words have similar vectors):\n",
            "'machine' and 'learning': 0.973 (closer to 1 = more similar)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###COMPARISON OF ALL METHODS"
      ],
      "metadata": {
        "id": "tDmtSou2uALs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNXwLjW1oPsO",
        "outputId": "ac84dc9f-59af-4443-b77c-3c20939ee532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPARISON OF ALL METHODS\n",
            "======================================================================\n",
            "\n",
            "METHOD          | DESCRIPTION                    | PROS                  | CONS\n",
            "----------------|--------------------------------|----------------------|----------------------\n",
            "Bag-of-Words    | Count word occurrences         | Simple, fast         | Ignores word order\n",
            "TF-IDF          | Weighted word importance       | Highlights key words | Still ignores order\n",
            "Word2Vec        | Dense vectors with meaning     | Captures semantics   | Needs more data\n",
            "\n",
            "\n",
            "WHEN TO USE WHAT:\n",
            "- Bag-of-Words: Simple classification tasks, quick prototypes\n",
            "- TF-IDF: Document search, finding important keywords\n",
            "- Word2Vec: When word meaning matters, similarity tasks\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARISON OF ALL METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison = \"\"\"\n",
        "METHOD          | DESCRIPTION                    | PROS                  | CONS\n",
        "----------------|--------------------------------|----------------------|----------------------\n",
        "Bag-of-Words    | Count word occurrences         | Simple, fast         | Ignores word order\n",
        "TF-IDF          | Weighted word importance       | Highlights key words | Still ignores order\n",
        "Word2Vec        | Dense vectors with meaning     | Captures semantics   | Needs more data\n",
        "\"\"\"\n",
        "\n",
        "print(comparison)\n",
        "\n",
        "print(\"\\nWHEN TO USE WHAT:\")\n",
        "print(\"- Bag-of-Words: Simple classification tasks, quick prototypes\")\n",
        "print(\"- TF-IDF: Document search, finding important keywords\")\n",
        "print(\"- Word2Vec: When word meaning matters, similarity tasks\")"
      ]
    }
  ]
}